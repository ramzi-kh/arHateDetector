{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "vYJcy_97m2q1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH4GZ2HXnav-"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/total.csv\", header=0)\n",
        "\n",
        "df['Tweet'] = df['Tweet'].astype(str)\n",
        "df['Class'] = df['Class'].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Class\"].value_counts()"
      ],
      "metadata": {
        "id": "lduv2YPVnpcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca619e2-98b4-4e34-f7d6-1664ee47cc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    22789\n",
              "1    10951\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j23qs0IbEhB7",
        "outputId": "24e4ccb5-3d3c-4577-f31c-99065ce272fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jMlCbw73p02u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING"
      ],
      "metadata": {
        "id": "U1c9ArfqanfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyarabic\n",
        "\n",
        "import re\n",
        "import pyarabic.araby as araby"
      ],
      "metadata": {
        "id": "ZFNQbpY1bHH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10de925d-5572-4947-e052-09a77c522e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip  install -U farasapy"
      ],
      "metadata": {
        "id": "dx5tNjIHgyoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d690996-6170-4d18-c98b-e791a668cc7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "\n",
        "\n",
        "stopwords = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n",
        "\n",
        "def remove_tashkeel(text):\n",
        "    return araby.strip_tashkeel(text)\n",
        "\n",
        "def remove_tatweel(text):\n",
        "    return araby.strip_tatweel(text)\n",
        "\n",
        "def normalization(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ي\", \"ى\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    return text\n",
        "\n",
        "def normalize_hamza(text):\n",
        "    return araby.normalize_hamza(text)\n",
        "\n",
        "def remove_usernames(text):\n",
        "    text = re.sub(\"@[\\w]*\", '', text)\n",
        "    return text\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    text = re.sub(\"#[\\w]*\", '', text)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[.,\\\"،\\/#!$%\\^&\\*;:{}=\\-_`~()?؟﴾﴿]',' ',text) #remove punctuation\n",
        "    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n",
        "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
        "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
        "    text = re.sub(r'[0-9]+',' ',text) #remove numbers\n",
        "    text = re.sub(r'[\\u0660-\\u0669]+',' ',text) #remove arabic numbers\n",
        "    text = re.sub(r\"_\",' ',text) #remove _\n",
        "    text = re.sub('\\s+', ' ', text).strip() #remove additional whitespaces\n",
        "    return text\n",
        "\n",
        "def remove_non_arabic_letters(text):\n",
        "    ENGLISH_CHARS =r'[a-zA-Z]+\\b(?<!urllink|mention)'\n",
        "    text = re.sub(ENGLISH_CHARS, \"\",  text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, stopwords):\n",
        "    word_list = text.split(' ')\n",
        "    filtered_words = [word for word in word_list if word not in stopwords]\n",
        "    text = ' '.join(filtered_words)\n",
        "    return text\n",
        "\n",
        "def remove_imoji(text):\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                    u\"\\U00002702-\\U000027B0\"\n",
        "                                    u\"\\U000024C2-\\U0001F251\"\n",
        "                                    \"]+\", flags=re.UNICODE)\n",
        "    text = re.sub(emoji_pattern, '', text)\n",
        "    return text\n",
        "\n",
        "#stemmer = FarasaStemmer()\n",
        "\n",
        "\n",
        "\n",
        "def clean_tweets(df):\n",
        "\n",
        "  tempArr = []\n",
        "  for line in df:\n",
        "\n",
        "    tweet = remove_tashkeel(line)\n",
        "    tweet = remove_tatweel(tweet)\n",
        "    tweet = normalization(tweet)\n",
        "    tweet = normalize_hamza(tweet)\n",
        "    tweet = remove_usernames(tweet)\n",
        "    tweet = remove_hashtags(tweet)\n",
        "    tweet = remove_non_arabic_letters(tweet)\n",
        "    tweet = remove_stopwords(tweet, stopwords)\n",
        "    tweet = remove_imoji(tweet)\n",
        "    tweet = clean_text(tweet)\n",
        "    #tweet = stemmer.stem(tweet)\n",
        "\n",
        "    tempArr.append(tweet)\n",
        "  return tempArr\n"
      ],
      "metadata": {
        "id": "cLTt9qiyam4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean data\n",
        "\n",
        "clean = clean_tweets(df[\"Tweet\"])\n",
        "clean = pd.DataFrame(clean)\n",
        "\n",
        "df[\"Tweet\"] = clean\n",
        "df['Tweet'] = df['Tweet'].astype(str)"
      ],
      "metadata": {
        "id": "fh0YKS7Pg-g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'Tweet':'texts'}, inplace=True)\n",
        "df.rename(columns={'Class':'data_labels'}, inplace=True)"
      ],
      "metadata": {
        "id": "CwJJZHWNujeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PARTITION"
      ],
      "metadata": {
        "id": "HCzwqlxha3uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.texts.values\n",
        "Y = df.data_labels.values\n",
        "\n",
        "X_train, X_val, Y_train, Y_val =train_test_split(X, Y, test_size=0.25, random_state=200)"
      ],
      "metadata": {
        "id": "i4X2CAewuoQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BEST ML ALGORITHM"
      ],
      "metadata": {
        "id": "uzwubvSvSNvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn \n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import random\n",
        "import joblib\n",
        "import pickle"
      ],
      "metadata": {
        "id": "NWx4Q_N86wUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = X_train\n",
        "y_train = Y_train\n",
        "\n",
        "x_test= X_val\n",
        "y_test = Y_val\n",
        "\n",
        "\n",
        "def detect_hate(my_classifier, name, x_train, y_train, x_test, y_test):\n",
        "    \n",
        "    print('parameters')\n",
        "    print('classifier:', my_classifier.__class__.__name__)\n",
        "    print('------------------------------------')\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(min_df=0.0001, max_df=0.95,\n",
        "                                 analyzer='word', lowercase=False,)),\n",
        "        ('clf', my_classifier),\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(x_train, y_train)\n",
        "    feature_names = pipeline.named_steps['vect'].get_feature_names()\n",
        "\n",
        "    y_predicted = pipeline.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "    # Print the classification report\n",
        "    print(metrics.classification_report(y_test, y_predicted,\n",
        "                                        ))\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "    print(cm)\n",
        "    print('# of features:', len(feature_names))\n",
        "    print('sample of features:', random.sample(feature_names, 40))\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
        "    recall =  recall_score(y_test, y_predicted, average='weighted')\n",
        "    f1 = f1_score(y_test, y_predicted, labels=None, pos_label=1, average='weighted', sample_weight=None)\n",
        "    \n",
        "    return name, accuracy, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "APmUOCbM62XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results = []\n",
        "\n",
        "classifiers = [LinearSVC()]\n",
        "\n",
        "for alg in classifiers:\n",
        "  alg_name = alg.__class__.__name__\n",
        "  r = detect_hate(alg, alg_name, x_train, y_train, x_test, y_test)\n",
        "  results.append(r)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh3VsDjn88zp",
        "outputId": "996fae5d-9b65-480a-f63a-11f0cb00ce84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters\n",
            "classifier: LinearSVC\n",
            "------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92      5662\n",
            "           1       0.87      0.78      0.82      2773\n",
            "\n",
            "    accuracy                           0.89      8435\n",
            "   macro avg       0.88      0.86      0.87      8435\n",
            "weighted avg       0.89      0.89      0.89      8435\n",
            "\n",
            "[[5337  325]\n",
            " [ 609 2164]]\n",
            "# of features: 19124\n",
            "sample of features: ['الاستعانه', 'ومعاه', 'فمك', 'فتلقىان', 'بكم', 'عقىد', 'افتخرت', 'خود', 'توزع', 'الزلمه', 'بىوتهم', 'وشتم', 'حقد', 'علىها', 'المدخلى', 'ىحكمها', 'المستفىد', 'بدماء', 'احذروه', 'واشف', 'ىاولاد', 'وتهرىبهم', 'الكافىه', 'طلبا', 'تتجاوز', 'رءاسه', 'بحكام', 'الصىدلىه', 'تفتخر', 'وجوب', 'تخلفهم', 'الدوام', 'هجوم', 'احمر', 'وبىت', 'وقد', 'احسب', 'مشهور', 'دمو', 'تخرب']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('{0:25}{1:>10}{2:>10}{3:>10}{4:>10}'.format('algorithm', 'accuracy', 'precision', 'recall', 'F1-score'))\n",
        "print('---------------------------------------------------------------------------')\n",
        "for r in results:\n",
        "    print('{0:25}{1:10.2f}{2:10.2f}{3:10.2f}{4:10.2f}'.format(r[0], r[1], r[2], r[3], r[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjNIQbll9wWz",
        "outputId": "86b95f9f-26cf-4001-8170-db51fa7fd86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm                  accuracy precision    recall  F1-score\n",
            "---------------------------------------------------------------------------\n",
            "LinearSVC                      0.89      0.89      0.89      0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE MODEL"
      ],
      "metadata": {
        "id": "BZ-Tw5Anmwtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"model\", \"wb\") as f:\n",
        "   pickle.dump(pipeline, f)\n"
      ],
      "metadata": {
        "id": "B2Dap18Vmnqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}